---
title: "Writeup"
author: "PLI"
date: "Saturday, January 24, 2015"
output: html_document
---
```{r optionSet, echo=FALSE}
## Setting global knit options
knitr::opts_chunk$set(echo=TRUE, message=FALSE, eval=TRUE, fig.width=12, fig.heigth=12)
````



# Introduction

The goal of this assignment is to predict the manner people perform barbell lifts. We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform this activity correctly and incorrectly. The outcome is recorded in the *classe* variable of the data set.

The training data will be use to train and validate the model and the test data to answer the second part of the assignment which consist of doing prediction on 20 samples.

* Training: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
* Testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Thanks to:
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

Read more: http://groupware.les.inf.puc-rio.br/har#sbia_paper_section#ixzz3OXdW9y00

We will start by loading the required libraries.
````{r setLibrary}
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(knitr))
suppressMessages(library(ggplot2))
suppressMessages(library(caret))
# Set seed for reproducibility
set.seed(32343)
```

# Reading and cleaning the data

 We will first load the training data, and transform strings *NA*, *#DIV/0!* and empty string by NAs.


```{r dataRead}
## Take care of #DIV/0! and NA when reading data
data <- read.csv("pml-training.csv", na.strings=c("NA", "#DIV/0!", ""), header=TRUE)
````

 We will then suppress useless variables: *row numbers*, *user names* and variables with *timestamp* as these should not be related to the outcome.
 
 From a quick explorations of the data we see that:
 
 * *new_window* and *num_window* seem to be related to some summarization variables
 * a lot of variables are full of NAs and are related with the preceding finding
 
 We decide to remove the two *window* variables and also the ones with more than 90% NAs.
 
```{r cleanData}
## Suppress row numbers, user_name and timestamps because they should be uncorrelated to the outcome
data <- select(data, -X, -user_name, -contains("timestamp"))

str(data[data$new_window=="no",1:20])
str(data[data$new_window=="yes",1:20])

## Suppress *window* variable
data <- select(data, -contains("window"))

## Remove columns with NA > 90% total
NaIndex<-(colSums(is.na(data)) < 0.9*nrow(data))
data<-data[,NaIndex]
dim(data)
````
Here is a plot of the 1000 random samples of 10 random variables names and the outcome.

```{r pairsPlot}
# Take 1000 samples
l<-dim(data)[1]
i<-sample(x=1:(l), size = 1000)
# Take 10 variables
w<-dim(data)[2]
j<-sample(x=1:(w), size = 10)
# Add classe variable for plot
s<-cbind(data[i,c(j,53)])
pal<-palette()
c<-pal[as.numeric(s$classe)]

pairs(s, col=c)
```


# Machine learning algorithm: Random Forest

We divide the training data in a train partition and a validation partition and define the trainControl method to be "oob" (out of bag) which is appropriate for random forest.

```{r preProcess}


## Create partition
inTrain = createDataPartition(data$classe, p = 3/4)[[1]]
training = data[ inTrain,]
validation = data[-inTrain,]

# TrainControl setting
# tCtrl<-trainControl(method = "cv")

tCtrl <- trainControl(method = "oob", number = 4, verboseIter = TRUE)

````
We have tried multiple training algorithms: rpart, pca, glm and finaly opted for random forest as it is the one with the best results. As we've reduced the dimensions of the problem from 155 to 53 variables the time required to train the model on a HP EliteBook 8560p is less than one hour.
To save computation time for the writing, we've decided to save the model on disk and reload it if required instead of recomputing it.

````{r rf}
modelFilename<-"rf-model-4oob.RData"
if (file.exists(modelFilename)) {
        load(file = modelFilename)
        } else {
                ## Time is 3234 sec for cv
                modelFit1<-train(training$classe ~ .,
                                 method="rf",
                                 preProcess = c("center", "scale"),
                                 data=training,
                                 trainControl = tCtrl)
                save(modelFit1, file=modelFilename)
                }
````

The confusion matrix on the training set gives us an estimate of the out of sample error.

```{r oosEstimate}
predictions <- predict(modelFit1,newdata=training)
confusionMatrix(training$classe, predictions)
```

As always, this estimate is optimistic (accuracy of 99.83% in this case) as we will see in the cross-validation section since the model has been constructed on this same data set.

# Other algorithms tested

Here are the other learning algorithms applied but not used because their accuracies weren't good enough.
We could have also used them in an "ensemble" implementation but the accuracy of the random forest algorithm was enough to make excellent prediction in this case.

````{r rpart, eval=FALSE}
modelFit <- train(training$classe ~ ., data=training, 
                  method="rpart",
                  preProcess = c("center", "scale"),
                  trainControl = tCtrl)
```

````{r pca-rf, eval=FALSE}
train2<-select(training, -classe)
preProc<-preProcess(train2, method="pca", thresh = 0.99)
trainPC<-predict(preProc, train2)
modelFit2<-train(training$classe ~ .,
                 method="rf",
                 data=trainPC)
test2<-select(validation, -classe)
testPC<-predict(preProc, test2)
````

````{r glm, eval=FALSE}
modelFit <- train(classe ~ ., data=training, 
                  method="glm",
                  preProcess = c("center", "scale"))

predictions <- predict(modelFit,newdata=validation)
```

# Cross validation

We now apply the model to the validation data set to cross-validate the out of sample error estimate:

```{r validation}
predictions <- predict(modelFit1,newdata=validation)
confusionMatrix(validation$classe, predictions)
```

The accuracy estimate is now 99.82% (0.18% error rate) with a confidence interval wider than the one on the training set as expected.

# Testing sample

Finally, we apply this model to the testing set to make our predictions for the second part of the assignment ...

```{r dataReadTest}
## Take care of #DIV/0! and NA when reading test
test <- read.csv("pml-testing.csv", na.strings=c("NA", "#DIV/0!", ""), header=TRUE)

test <- select(test, -X, -user_name, -contains("timestamp"))

## Suppress *window* variable
test <- select(test, -contains("window"))

## Remove columns with NA > 90% total
NaIndex<-(colSums(is.na(test)) < 0.9*nrow(test))
test<-test[,NaIndex]
dim(test)

predictions <- predict(modelFit1,newdata=test[,-53])

predictions
````


```{r submission, eval=FALSE, echo=FALSE}
# Just to make the files for submissions
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
setwd("./Submission/")
pml_write_files(as.character(predictions))
```

